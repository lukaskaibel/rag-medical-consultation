{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28878211",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables from .env at the repository root\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3637bdcc",
   "metadata": {},
   "source": [
    "# Match Documents to Questions\n",
    "\n",
    "Matching documents to questions using the references for each question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef9a21e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from src.utils.pickle_utils import for_each_pickle_file\n",
    "from src.utils.document_utils import find_matching_docs\n",
    "import os\n",
    "\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dbfdd59f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Pickle files: 100%|██████████| 8/8 [01:49<00:00, 13.65s/it]\n"
     ]
    }
   ],
   "source": [
    "def add_matching_documents_from_pkl_file(filename, df):\n",
    "    question_answers_dataset = pd.read_pickle(\"data/datasets/question_answers_dataset_raw.pkl\")\n",
    "    documents = df[\"document\"].tolist()\n",
    "    question_answers_dataset[\"documents\"] = question_answers_dataset[\"references\"].apply(\n",
    "        lambda refs: find_matching_docs(refs, documents, threshold=0.5)\n",
    "    )\n",
    "    clean_name = os.path.splitext(os.path.basename(filename))[0]\n",
    "    question_answers_dataset.to_pickle(f\"data/qa_with_docs/question_answers_{clean_name}_dataset.pkl\")\n",
    "\n",
    "for_each_pickle_file(\"data/preprocessed_documents\", add_matching_documents_from_pkl_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d18093c",
   "metadata": {},
   "source": [
    "## Dataset Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "985e01a0",
   "metadata": {},
   "source": [
    "### Number of relevant chunks per question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b766af17",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'for_each_pickle_file' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 20\u001b[39m\n\u001b[32m     13\u001b[39m         records.append({\n\u001b[32m     14\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mfile\u001b[39m\u001b[33m\"\u001b[39m: filename,\n\u001b[32m     15\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mdoc_count\u001b[39m\u001b[33m\"\u001b[39m: cnt\n\u001b[32m     16\u001b[39m         })\n\u001b[32m     18\u001b[39m \u001b[38;5;66;03m# hier wird wie gewohnt über alle Pickles iteriert,\u001b[39;00m\n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m# our callback füllt nur 'records'\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m \u001b[43mfor_each_pickle_file\u001b[49m(\u001b[33m\"\u001b[39m\u001b[33mdata/qa_with_docs\u001b[39m\u001b[33m\"\u001b[39m, validate_qa_with_docs)\n\u001b[32m     22\u001b[39m \u001b[38;5;66;03m# nach der Schleife haben wir alles in records – jetzt einmaliges Boxplot\u001b[39;00m\n\u001b[32m     23\u001b[39m plot_df = pd.DataFrame(records)\n",
      "\u001b[31mNameError\u001b[39m: name 'for_each_pickle_file' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# dieser globale Container sammelt alle Counts aus jedem einzelnen Pickle\n",
    "records = []\n",
    "\n",
    "def validate_qa_with_docs(filename, df):\n",
    "    # Länge der Liste df[\"documents\"] pro Frage\n",
    "    doc_counts = df[\"documents\"].apply(len)\n",
    "    # für jede Count ein Record mit Dateiname\n",
    "    for cnt in doc_counts:\n",
    "        records.append({\n",
    "            \"file\": filename,\n",
    "            \"doc_count\": cnt\n",
    "        })\n",
    "\n",
    "# hier wird wie gewohnt über alle Pickles iteriert,\n",
    "# our callback füllt nur 'records'\n",
    "for_each_pickle_file(\"data/qa_with_docs\", validate_qa_with_docs)\n",
    "\n",
    "# nach der Schleife haben wir alles in records – jetzt einmaliges Boxplot\n",
    "plot_df = pd.DataFrame(records)\n",
    "\n",
    "sns.set(style=\"whitegrid\")\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.boxplot(\n",
    "    data=plot_df,\n",
    "    x=\"file\",\n",
    "    y=\"doc_count\",\n",
    "    showfliers=False        # Ausreißer ausblenden, optional\n",
    ")\n",
    "plt.title(\"Vergleich: Anzahl Dokumente pro Frage für alle Pickle-Dateien\")\n",
    "plt.xlabel(\"Pickle-Datei\")\n",
    "plt.ylabel(\"Anzahl Dokumente pro Frage\")\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d55aa0e",
   "metadata": {},
   "source": [
    "### Check Reference to Document Mapping\n",
    "\n",
    "Checks if the documents mapped to the question contain all the references from that question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ae46a62b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Pickle files:  12%|█▎        | 1/8 [01:16<08:54, 76.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fquestion_answers_docs_line_1_0_dataset.pkl: 0 rows have missing reference matches.\n",
      "Empty DataFrame\n",
      "Columns: [question, references, missing_refs, documents]\n",
      "Index: []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Pickle files:  25%|██▌       | 2/8 [02:32<07:38, 76.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fquestion_answers_docs_passage_1_0_dataset.pkl: 0 rows have missing reference matches.\n",
      "Empty DataFrame\n",
      "Columns: [question, references, missing_refs, documents]\n",
      "Index: []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Pickle files:  38%|███▊      | 3/8 [03:47<06:19, 75.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fquestion_answers_docs_word_100_20_dataset.pkl: 0 rows have missing reference matches.\n",
      "Empty DataFrame\n",
      "Columns: [question, references, missing_refs, documents]\n",
      "Index: []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Pickle files:  50%|█████     | 4/8 [05:03<05:03, 75.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fquestion_answers_docs_word_31_6_dataset.pkl: 0 rows have missing reference matches.\n",
      "Empty DataFrame\n",
      "Columns: [question, references, missing_refs, documents]\n",
      "Index: []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Pickle files:  62%|██████▎   | 5/8 [06:20<03:48, 76.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fquestion_answers_docs_word_400_80_dataset.pkl: 0 rows have missing reference matches.\n",
      "Empty DataFrame\n",
      "Columns: [question, references, missing_refs, documents]\n",
      "Index: []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Pickle files:  75%|███████▌  | 6/8 [07:37<02:32, 76.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fquestion_answers_docs_word_41_8_dataset.pkl: 0 rows have missing reference matches.\n",
      "Empty DataFrame\n",
      "Columns: [question, references, missing_refs, documents]\n",
      "Index: []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Pickle files:  88%|████████▊ | 7/8 [08:53<01:16, 76.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fquestion_answers_docs_word_50_10_dataset.pkl: 0 rows have missing reference matches.\n",
      "Empty DataFrame\n",
      "Columns: [question, references, missing_refs, documents]\n",
      "Index: []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Pickle files: 100%|██████████| 8/8 [10:11<00:00, 76.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fquestion_answers_docs_word_800_160_dataset.pkl: 0 rows have missing reference matches.\n",
      "Empty DataFrame\n",
      "Columns: [question, references, missing_refs, documents]\n",
      "Index: []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def find_missing_reference_matches_in_df(filename, df):\n",
    "    # Show all columns, rows, and avoid truncating column content\n",
    "    pd.set_option('display.max_columns', None)\n",
    "    pd.set_option('display.max_rows', None)\n",
    "    pd.set_option('display.max_colwidth', None)\n",
    "    pd.set_option('display.width', None)\n",
    "\n",
    "    def find_missing_refs(references, documents):\n",
    "        missing_refs = []\n",
    "\n",
    "        for ref in references:\n",
    "            if len(find_matching_docs([ref], documents, threshold=0.5)) == 0:\n",
    "                missing_refs.append(ref)\n",
    "\n",
    "        return missing_refs\n",
    "\n",
    "    # Apply function to extract missing references\n",
    "    df[\"missing_refs\"] = df.apply(\n",
    "        lambda row: find_missing_refs(row[\"references\"], row[\"documents\"]), axis=1\n",
    "    )\n",
    "\n",
    "    # Only keep rows with missing references\n",
    "    failed_rows = df[df[\"missing_refs\"].apply(len) > 0]\n",
    "\n",
    "    print(f\"f{filename}: {len(failed_rows)} rows have missing reference matches.\")\n",
    "    print(failed_rows[[\"question\", \"references\", \"missing_refs\", \"documents\"]])\n",
    "\n",
    "for_each_pickle_file(\"data/qa_with_docs\", find_missing_reference_matches_in_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e4c26b",
   "metadata": {},
   "source": [
    "## Flatten Question Variations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "10254962",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Flattening Question Variations: 100%|██████████| 61/61 [00:00<00:00, 230.50it/s]\n",
      "Flattening Question Variations: 100%|██████████| 61/61 [00:00<00:00, 231.55it/s]\n",
      "Flattening Question Variations: 100%|██████████| 61/61 [00:00<00:00, 166.04it/s]\n",
      "Flattening Question Variations: 100%|██████████| 61/61 [00:00<00:00, 228.24it/s]\n",
      "Flattening Question Variations: 100%|██████████| 61/61 [00:00<00:00, 233.95it/s]\n",
      "Flattening Question Variations: 100%|██████████| 61/61 [00:00<00:00, 233.49it/s]\n",
      "Flattening Question Variations: 100%|██████████| 61/61 [00:00<00:00, 233.28it/s]\n",
      "Flattening Question Variations: 100%|██████████| 61/61 [00:00<00:00, 229.72it/s]\n",
      "Processing Pickle files: 100%|██████████| 8/8 [00:02<00:00,  3.10it/s]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "def flatten(filename, df):\n",
    "    new_rows = []\n",
    "\n",
    "    for _, row in tqdm(df.iterrows(), total=len(df), desc=\"Flattening Question Variations\"):\n",
    "        # 0) Include the original row itself\n",
    "        orig = row.copy()\n",
    "        orig[\"variant\"]       = \"default\"\n",
    "        orig[\"prev_messages\"] = []\n",
    "        new_rows.append(orig)\n",
    "\n",
    "        # 1) Unpack each variation\n",
    "        variations = json.loads(row[\"variations\"])\n",
    "        for variation_key, variation_val in variations.items():\n",
    "            nr = row.copy()\n",
    "            nr[\"variant\"] = variation_key\n",
    "\n",
    "            if variation_key == \"contextual\" and isinstance(variation_val, list):\n",
    "                nr[\"question\"]      = variation_val[-1][\"message\"]\n",
    "                nr[\"prev_messages\"] = [m[\"message\"] for m in variation_val[:-1]]\n",
    "            else:\n",
    "                nr[\"question\"]      = variation_val\n",
    "                nr[\"prev_messages\"] = []\n",
    "\n",
    "            new_rows.append(nr)\n",
    "\n",
    "    # 2) Build your flattened DataFrame and drop 'variations'\n",
    "    flat_df = (\n",
    "        pd.DataFrame(new_rows)\n",
    "        .reset_index(drop=True)\n",
    "        .drop(columns=[\"variations\"])\n",
    "    )\n",
    "    # Strip .pkl if it exists\n",
    "    clean_name = os.path.splitext(os.path.basename(filename))[0]\n",
    "    flat_df.to_pickle(f\"data/qa_with_docs_flat/{clean_name}_flat.pkl\")\n",
    "\n",
    "for_each_pickle_file(\"data/qa_with_docs\", flatten)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b71662",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79db83b6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
