{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0dcd96e4",
   "metadata": {},
   "source": [
    "# Retrieval Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "36ee93f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install \"haystack-ai>=2.15.0rc1\"\n",
    "# %pip install ragas-haystack\n",
    "# %pip install nltk\n",
    "# %pip install openai\n",
    "# %pip install pandas\n",
    "# %pip install ragas-haystack\n",
    "# %pip install \"sentence-transformers>=3.0.0\"\n",
    "# %pip install hf_xet\n",
    "# %pip install \"ollama-haystack==2.4.2\"\n",
    "# %pip install tqdm # For Progress Bar\n",
    "# %pip install einops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ef9a21e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from haystack.document_stores.in_memory import InMemoryDocumentStore\n",
    "import importlib\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "tqdm.pandas()\n",
    "import logging\n",
    "from utils.pickle_utils import for_each_pickle_file\n",
    "logging.basicConfig(\n",
    "    level=logging.WARNING,\n",
    "    format='%(asctime)s %(levelname)s %(message)s',\n",
    "    datefmt='%H:%M:%S'\n",
    ")\n",
    "logging.getLogger(\"haystack\").setLevel(logging.WARNING)\n",
    "\n",
    "os.environ[\"SENTENCE_TRANSFORMERS_HOME\"] = \"./model-assets/sentence-transformers\"\n",
    "os.environ[\"HF_HUB_CACHE\"] = \"./model-assets/hugging-face\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "25347f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from config.secret import OPENAI_API_KEY\n",
    "# os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n",
    "\n",
    "os.environ[\"EMBEDDING_MODEL_NAME\"] = \"Qwen/Qwen3-Embedding-4B\"\n",
    "os.environ[\"RERANKING_MODEL_NAME\"] = \"Qwen/Qwen3-Reranker-4B\"\n",
    "os.environ[\"LLM_NAME\"] = \"gemma3:12b\"\n",
    "os.environ[\"LLM_CONTEXT_SIZE\"] = \"8192\"\n",
    "\n",
    "embedder = \"Qwen/Qwen3-Embedding-4B\"\n",
    "reranker = \"Qwen/Qwen3-Reranker-4B\"\n",
    "\n",
    "TOP_K_VALUES = [5, 10, 20, 40, 80]\n",
    "NUMBER_OF_QUESTIONS_IN_EVAL = 600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "08db30eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pipelines.evaluation.base_retrieval_eval_pipeline import get_base_retrieval_eval_pipeline\n",
    "from pipelines.evaluation.hybrid_retrieval_eval_pipeline import get_hybrid_retrieval_eval_pipeline\n",
    "from pipelines.evaluation.random_retrieval_eval_pipeline import get_random_retrieval_eval_pipeline\n",
    "from models import EmbeddingModelConfig, EmbeddingModelProvider, RerankingModelConfig, RerankingModelProvider\n",
    "\n",
    "def get_test_cases(splitting_strategy: str):\n",
    "    base_indexing_store = InMemoryDocumentStore.load_from_disk(f\"data/document_stores/{os.environ['EMBEDDING_MODEL_NAME']}/base/{splitting_strategy}_indexing_store.json\")\n",
    "    context_indexing_store = InMemoryDocumentStore.load_from_disk(f\"data/document_stores/{os.environ['EMBEDDING_MODEL_NAME']}/context/{os.environ['LLM_NAME']}/{splitting_strategy}_indexing_store.json\")\n",
    "\n",
    "    test_cases = [\n",
    "        {\n",
    "            \"name\": \"Random\",\n",
    "            \"pipeline\": get_random_retrieval_eval_pipeline(base_indexing_store),\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"Basic RAG\",\n",
    "            \"pipeline\": get_base_retrieval_eval_pipeline(\n",
    "                base_indexing_store, \n",
    "                EmbeddingModelConfig(name=embedder, provider=EmbeddingModelProvider.SENTENCE_TRANSFORMER), \n",
    "                RerankingModelConfig(name=reranker, provider=RerankingModelProvider.HUGGING_FACE),\n",
    "                rewriting_model_config=None,\n",
    "            ),\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"Hybrid RAG\",\n",
    "            \"pipeline\": get_hybrid_retrieval_eval_pipeline(\n",
    "                base_indexing_store,\n",
    "                EmbeddingModelConfig(name=embedder, provider=EmbeddingModelProvider.SENTENCE_TRANSFORMER), \n",
    "                RerankingModelConfig(name=reranker, provider=RerankingModelProvider.HUGGING_FACE),\n",
    "                rewriter_model_config=None,\n",
    "            ),\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"Contextual RAG\",\n",
    "            \"pipeline\": get_base_retrieval_eval_pipeline(\n",
    "                context_indexing_store, \n",
    "                EmbeddingModelConfig(name=embedder, provider=EmbeddingModelProvider.SENTENCE_TRANSFORMER), \n",
    "                RerankingModelConfig(name=reranker, provider=RerankingModelProvider.HUGGING_FACE),\n",
    "                rewriting_model_config=None,\n",
    "            ),\n",
    "        },\n",
    "    ]\n",
    "    \n",
    "    return test_cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "575791fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Pickle files:   0%|          | 0/6 [00:00<?, ?it/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13f3a251c94f4e57b2f69e56e175523e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing rows:   0%|          | 0/600 [00:14<?, ?row/s]\n",
      "Processing Pickle files:   0%|          | 0/6 [00:14<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 77\u001b[39m\n\u001b[32m     74\u001b[39m         os.makedirs(os.path.dirname(save_path), exist_ok=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     75\u001b[39m         df.to_pickle(save_path)\n\u001b[32m---> \u001b[39m\u001b[32m77\u001b[39m \u001b[43mfor_each_pickle_file\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdata/qa_with_docs_flat\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_retrieval_eval\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/srv/data/pia-rag-eval/utils/pickle_utils.py:26\u001b[39m, in \u001b[36mfor_each_pickle_file\u001b[39m\u001b[34m(dir_path, callback)\u001b[39m\n\u001b[32m     24\u001b[39m file_path = os.path.join(dir_path, filename)\n\u001b[32m     25\u001b[39m df = pd.read_pickle(file_path)\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m \u001b[43mcallback\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 30\u001b[39m, in \u001b[36mrun_retrieval_eval\u001b[39m\u001b[34m(filename, df)\u001b[39m\n\u001b[32m     27\u001b[39m relevant_documents = row[\u001b[33m\"\u001b[39m\u001b[33mdocuments\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m     28\u001b[39m question = row[\u001b[33m\"\u001b[39m\u001b[33mquestion\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m test_cases = \u001b[43mget_test_cases\u001b[49m\u001b[43m(\u001b[49m\u001b[43msplitting_strategy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     32\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m test_case \u001b[38;5;129;01min\u001b[39;00m test_cases:\n\u001b[32m     33\u001b[39m     pipeline = test_case[\u001b[33m\"\u001b[39m\u001b[33mpipeline\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 17\u001b[39m, in \u001b[36mget_test_cases\u001b[39m\u001b[34m(splitting_strategy)\u001b[39m\n\u001b[32m      7\u001b[39m base_indexing_store = InMemoryDocumentStore.load_from_disk(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mdata/document_stores/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mos.environ[\u001b[33m'\u001b[39m\u001b[33mEMBEDDING_MODEL_NAME\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/base/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msplitting_strategy\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_indexing_store.json\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      8\u001b[39m context_indexing_store = InMemoryDocumentStore.load_from_disk(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mdata/document_stores/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mos.environ[\u001b[33m'\u001b[39m\u001b[33mEMBEDDING_MODEL_NAME\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/context/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mos.environ[\u001b[33m'\u001b[39m\u001b[33mLLM_NAME\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msplitting_strategy\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_indexing_store.json\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     10\u001b[39m test_cases = [\n\u001b[32m     11\u001b[39m     {\n\u001b[32m     12\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mname\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mRandom\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     13\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mpipeline\u001b[39m\u001b[33m\"\u001b[39m: get_random_retrieval_eval_pipeline(base_indexing_store),\n\u001b[32m     14\u001b[39m     },\n\u001b[32m     15\u001b[39m     {\n\u001b[32m     16\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mname\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mBasic RAG\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mpipeline\u001b[39m\u001b[33m\"\u001b[39m: \u001b[43mget_base_retrieval_eval_pipeline\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m            \u001b[49m\u001b[43mbase_indexing_store\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     19\u001b[39m \u001b[43m            \u001b[49m\u001b[43mEmbeddingModelConfig\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[43membedder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprovider\u001b[49m\u001b[43m=\u001b[49m\u001b[43mEmbeddingModelProvider\u001b[49m\u001b[43m.\u001b[49m\u001b[43mSENTENCE_TRANSFORMER\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     20\u001b[39m \u001b[43m            \u001b[49m\u001b[43mRerankingModelConfig\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreranker\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprovider\u001b[49m\u001b[43m=\u001b[49m\u001b[43mRerankingModelProvider\u001b[49m\u001b[43m.\u001b[49m\u001b[43mHUGGING_FACE\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrewriting_model_config\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[32m     23\u001b[39m     },\n\u001b[32m     24\u001b[39m     {\n\u001b[32m     25\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mname\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mHybrid RAG\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     26\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mpipeline\u001b[39m\u001b[33m\"\u001b[39m: get_hybrid_retrieval_eval_pipeline(\n\u001b[32m     27\u001b[39m             base_indexing_store,\n\u001b[32m     28\u001b[39m             EmbeddingModelConfig(name=embedder, provider=EmbeddingModelProvider.SENTENCE_TRANSFORMER), \n\u001b[32m     29\u001b[39m             RerankingModelConfig(name=reranker, provider=RerankingModelProvider.HUGGING_FACE),\n\u001b[32m     30\u001b[39m             rewriter_model_config=\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m     31\u001b[39m         ),\n\u001b[32m     32\u001b[39m     },\n\u001b[32m     33\u001b[39m     {\n\u001b[32m     34\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mname\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mContextual RAG\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     35\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mpipeline\u001b[39m\u001b[33m\"\u001b[39m: get_base_retrieval_eval_pipeline(\n\u001b[32m     36\u001b[39m             context_indexing_store, \n\u001b[32m     37\u001b[39m             EmbeddingModelConfig(name=embedder, provider=EmbeddingModelProvider.SENTENCE_TRANSFORMER), \n\u001b[32m     38\u001b[39m             RerankingModelConfig(name=reranker, provider=RerankingModelProvider.HUGGING_FACE),\n\u001b[32m     39\u001b[39m             rewriting_model_config=\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m     40\u001b[39m         ),\n\u001b[32m     41\u001b[39m     },\n\u001b[32m     42\u001b[39m ]\n\u001b[32m     44\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m test_cases\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/srv/data/pia-rag-eval/pipelines/evaluation/base_retrieval_eval_pipeline.py:14\u001b[39m, in \u001b[36mget_base_retrieval_eval_pipeline\u001b[39m\u001b[34m(base_indexing_store, embedding_model_config, reranking_model_config, rewriting_model_config)\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_base_retrieval_eval_pipeline\u001b[39m(\n\u001b[32m      9\u001b[39m     base_indexing_store: InMemoryDocumentStore,\n\u001b[32m     10\u001b[39m     embedding_model_config: EmbeddingModelConfig,\n\u001b[32m     11\u001b[39m     reranking_model_config: RerankingModelConfig,\n\u001b[32m     12\u001b[39m     rewriting_model_config: RewriterModelConfig,\n\u001b[32m     13\u001b[39m ) -> Pipeline:\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m     base_retrieval_pipeline = \u001b[43mpipelines\u001b[49m\u001b[43m.\u001b[49m\u001b[43mretrieval_pipelines\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbase_retrieval_pipeline\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_base_retrieval_pipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_indexing_store\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding_model_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreranking_model_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrewriting_model_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     16\u001b[39m     map_evaluator = DocumentMAPEvaluator()\n\u001b[32m     17\u001b[39m     mrr_evaluator = DocumentMRREvaluator()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/srv/data/pia-rag-eval/pipelines/retrieval_pipelines/base_retrieval_pipeline.py:29\u001b[39m, in \u001b[36mget_base_retrieval_pipeline\u001b[39m\u001b[34m(document_store, embedding_model_config, reranking_model_config, rewriter_model_config)\u001b[39m\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m embedding_model_config.provider == EmbeddingModelProvider.SENTENCE_TRANSFORMER:\n\u001b[32m     23\u001b[39m     query_embedder = SentenceTransformersTextEmbedder(\n\u001b[32m     24\u001b[39m         model=embedding_model_config.name,\n\u001b[32m     25\u001b[39m         prefix=\u001b[33m\"\u001b[39m\u001b[33mInstruct: Given a question, retrieve relevant passages that answer the question\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mQuestion:\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     26\u001b[39m         device=ComponentDevice.from_single(Device.gpu(\u001b[38;5;28mid\u001b[39m=\u001b[32m2\u001b[39m)),\n\u001b[32m     27\u001b[39m         model_kwargs={\u001b[33m\"\u001b[39m\u001b[33mtorch_dtype\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mfloat16\u001b[39m\u001b[33m\"\u001b[39m}\n\u001b[32m     28\u001b[39m     )\n\u001b[32m---> \u001b[39m\u001b[32m29\u001b[39m     \u001b[43mquery_embedder\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwarm_up\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     30\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m embedding_model_config.provider == EmbeddingModelProvider.OPENAI:\n\u001b[32m     31\u001b[39m     query_embedder = OpenAITextEmbedder(\n\u001b[32m     32\u001b[39m         model=embedding_model_config.name\n\u001b[32m     33\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/srv/data/pia-rag-eval/.venv/lib/python3.11/site-packages/haystack/components/embedders/sentence_transformers_text_embedder.py:192\u001b[39m, in \u001b[36mSentenceTransformersTextEmbedder.warm_up\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    188\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    189\u001b[39m \u001b[33;03mInitializes the component.\u001b[39;00m\n\u001b[32m    190\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    191\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.embedding_backend \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m192\u001b[39m     \u001b[38;5;28mself\u001b[39m.embedding_backend = \u001b[43m_SentenceTransformersEmbeddingBackendFactory\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_embedding_backend\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    193\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    194\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto_torch_str\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    195\u001b[39m \u001b[43m        \u001b[49m\u001b[43mauth_token\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    196\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    197\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    198\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtruncate_dim\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtruncate_dim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    199\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    200\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtokenizer_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtokenizer_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    201\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconfig_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconfig_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    202\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    203\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    204\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.tokenizer_kwargs \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.tokenizer_kwargs.get(\u001b[33m\"\u001b[39m\u001b[33mmodel_max_length\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    205\u001b[39m         \u001b[38;5;28mself\u001b[39m.embedding_backend.model.max_seq_length = \u001b[38;5;28mself\u001b[39m.tokenizer_kwargs[\u001b[33m\"\u001b[39m\u001b[33mmodel_max_length\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/srv/data/pia-rag-eval/.venv/lib/python3.11/site-packages/haystack/components/embedders/backends/sentence_transformers_backend.py:38\u001b[39m, in \u001b[36m_SentenceTransformersEmbeddingBackendFactory.get_embedding_backend\u001b[39m\u001b[34m(model, device, auth_token, trust_remote_code, local_files_only, truncate_dim, model_kwargs, tokenizer_kwargs, config_kwargs, backend)\u001b[39m\n\u001b[32m     36\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m embedding_backend_id \u001b[38;5;129;01min\u001b[39;00m _SentenceTransformersEmbeddingBackendFactory._instances:\n\u001b[32m     37\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _SentenceTransformersEmbeddingBackendFactory._instances[embedding_backend_id]\n\u001b[32m---> \u001b[39m\u001b[32m38\u001b[39m embedding_backend = \u001b[43m_SentenceTransformersEmbeddingBackend\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     39\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     40\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     41\u001b[39m \u001b[43m    \u001b[49m\u001b[43mauth_token\u001b[49m\u001b[43m=\u001b[49m\u001b[43mauth_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     42\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     43\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     44\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtruncate_dim\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtruncate_dim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     45\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     46\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtokenizer_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtokenizer_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     47\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconfig_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     48\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     49\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     50\u001b[39m _SentenceTransformersEmbeddingBackendFactory._instances[embedding_backend_id] = embedding_backend\n\u001b[32m     51\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m embedding_backend\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/srv/data/pia-rag-eval/.venv/lib/python3.11/site-packages/haystack/components/embedders/backends/sentence_transformers_backend.py:74\u001b[39m, in \u001b[36m_SentenceTransformersEmbeddingBackend.__init__\u001b[39m\u001b[34m(self, model, device, auth_token, trust_remote_code, local_files_only, truncate_dim, model_kwargs, tokenizer_kwargs, config_kwargs, backend)\u001b[39m\n\u001b[32m     59\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(  \u001b[38;5;66;03m# pylint: disable=too-many-positional-arguments\u001b[39;00m\n\u001b[32m     60\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m     61\u001b[39m     model: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     70\u001b[39m     backend: Literal[\u001b[33m\"\u001b[39m\u001b[33mtorch\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33monnx\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mopenvino\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[33m\"\u001b[39m\u001b[33mtorch\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     71\u001b[39m ):\n\u001b[32m     72\u001b[39m     sentence_transformers_import.check()\n\u001b[32m---> \u001b[39m\u001b[32m74\u001b[39m     \u001b[38;5;28mself\u001b[39m.model = \u001b[43mSentenceTransformer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     75\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     76\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     77\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mauth_token\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresolve_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mauth_token\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     78\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     79\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     80\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtruncate_dim\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtruncate_dim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     81\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     82\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtokenizer_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtokenizer_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     83\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconfig_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     84\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     85\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/srv/data/pia-rag-eval/.venv/lib/python3.11/site-packages/sentence_transformers/SentenceTransformer.py:348\u001b[39m, in \u001b[36mSentenceTransformer.__init__\u001b[39m\u001b[34m(self, model_name_or_path, modules, device, prompts, default_prompt_name, similarity_fn_name, cache_folder, trust_remote_code, revision, local_files_only, token, use_auth_token, truncate_dim, model_kwargs, tokenizer_kwargs, config_kwargs, model_card_data, backend)\u001b[39m\n\u001b[32m    345\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[32m    346\u001b[39m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m348\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    349\u001b[39m \u001b[38;5;28mself\u001b[39m.is_hpu_graph_enabled = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    351\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.default_prompt_name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.default_prompt_name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.prompts:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/srv/data/pia-rag-eval/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1355\u001b[39m, in \u001b[36mModule.to\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1352\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1353\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1355\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/srv/data/pia-rag-eval/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:915\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    913\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[32m    914\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.children():\n\u001b[32m--> \u001b[39m\u001b[32m915\u001b[39m         \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    917\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[32m    918\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[32m    919\u001b[39m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[32m    920\u001b[39m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    925\u001b[39m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[32m    926\u001b[39m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/srv/data/pia-rag-eval/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:915\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    913\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[32m    914\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.children():\n\u001b[32m--> \u001b[39m\u001b[32m915\u001b[39m         \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    917\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[32m    918\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[32m    919\u001b[39m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[32m    920\u001b[39m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    925\u001b[39m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[32m    926\u001b[39m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "    \u001b[31m[... skipping similar frames: Module._apply at line 915 (3 times)]\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/srv/data/pia-rag-eval/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:915\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    913\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[32m    914\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.children():\n\u001b[32m--> \u001b[39m\u001b[32m915\u001b[39m         \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    917\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[32m    918\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[32m    919\u001b[39m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[32m    920\u001b[39m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    925\u001b[39m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[32m    926\u001b[39m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/srv/data/pia-rag-eval/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:942\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    938\u001b[39m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[32m    939\u001b[39m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[32m    940\u001b[39m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[32m    941\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m--> \u001b[39m\u001b[32m942\u001b[39m     param_applied = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    943\u001b[39m p_should_use_set_data = compute_should_use_set_data(param, param_applied)\n\u001b[32m    945\u001b[39m \u001b[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/srv/data/pia-rag-eval/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1341\u001b[39m, in \u001b[36mModule.to.<locals>.convert\u001b[39m\u001b[34m(t)\u001b[39m\n\u001b[32m   1334\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t.dim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[32m4\u001b[39m, \u001b[32m5\u001b[39m):\n\u001b[32m   1335\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m t.to(\n\u001b[32m   1336\u001b[39m             device,\n\u001b[32m   1337\u001b[39m             dtype \u001b[38;5;28;01mif\u001b[39;00m t.is_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t.is_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1338\u001b[39m             non_blocking,\n\u001b[32m   1339\u001b[39m             memory_format=convert_to_format,\n\u001b[32m   1340\u001b[39m         )\n\u001b[32m-> \u001b[39m\u001b[32m1341\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1342\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1343\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1344\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1345\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1346\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1347\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) == \u001b[33m\"\u001b[39m\u001b[33mCannot copy out of meta tensor; no data!\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "now = datetime.now()\n",
    "\n",
    "def run_retrieval_eval(filename, df):\n",
    "    import config.prompt\n",
    "    importlib.reload(config.prompt)\n",
    "\n",
    "    import re\n",
    "\n",
    "    match = re.search(r\"answers_(.*?)_dataset\", filename)\n",
    "    if match:\n",
    "        splitting_strategy = match.group(1)\n",
    "    else:\n",
    "        splitting_strategy = None\n",
    "\n",
    "    # 1) Filter out the null‐question rows\n",
    "    df_nonnull = df[df[\"question\"].notnull()]\n",
    "\n",
    "    df_shuffled = df_nonnull.sample(n=NUMBER_OF_QUESTIONS_IN_EVAL, random_state=42).reset_index(drop=True)\n",
    "\n",
    "    for top_k in TOP_K_VALUES:\n",
    "        for index, row in tqdm(\n",
    "            df_shuffled.iterrows(),\n",
    "            total=len(df_shuffled),\n",
    "            desc=\"Processing rows\",\n",
    "            unit=\"row\"\n",
    "        ):\n",
    "            relevant_documents = row[\"documents\"]\n",
    "            question = row[\"question\"]\n",
    "\n",
    "            test_cases = get_test_cases(splitting_strategy)\n",
    "\n",
    "            for test_case in test_cases:\n",
    "                pipeline = test_case[\"pipeline\"]\n",
    "                request_payload = {\n",
    "                    \"retriever\": {\n",
    "                        \"top_k\": math.ceil(top_k / 2) if \"Hybrid\" in test_case[\"name\"] else top_k\n",
    "                    },\n",
    "                    \"map_evaluator\": {\n",
    "                        \"ground_truth_documents\": [relevant_documents],\n",
    "                    },\n",
    "                    \"mrr_evaluator\": {\n",
    "                        \"ground_truth_documents\": [relevant_documents],\n",
    "                    },\n",
    "                    \"recall_evaluator\": {\n",
    "                        \"ground_truth_documents\": [relevant_documents],\n",
    "                    }\n",
    "                }\n",
    "                if \"query_embedder\" in pipeline.graph.nodes:\n",
    "                    request_payload[\"query_embedder\"] = {\n",
    "                        \"text\": question,\n",
    "                    }\n",
    "                if \"reranker\" in pipeline.graph.nodes:\n",
    "                    request_payload[\"reranker\"] = {\n",
    "                        \"query\": question,\n",
    "                        \"top_k\": top_k,\n",
    "                    }\n",
    "                if \"bm25_retriever\" in pipeline.graph.nodes:\n",
    "                    request_payload[\"bm25_retriever\"] = {\n",
    "                        \"query\": question,\n",
    "                        \"top_k\": math.floor(top_k / 2) if \"Hybrid\" in test_case[\"name\"] else top_k,\n",
    "                    }\n",
    "                result = pipeline.run(request_payload)\n",
    "\n",
    "                map_score = result.get(\"map_evaluator\", {}).get(\"score\", {})\n",
    "                mrr_score = result.get(\"mrr_evaluator\", {}).get(\"score\", {})\n",
    "                recall_score = result.get(\"recall_evaluator\", {}).get(\"score\", {})\n",
    "\n",
    "                df.at[index, f\"{test_case['name']}_map\"] = map_score\n",
    "                df.at[index, f\"{test_case['name']}_mrr\"] = mrr_score\n",
    "                df.at[index, f\"{test_case['name']}_recall\"] = recall_score\n",
    "\n",
    "        embedding_model_name = os.environ['EMBEDDING_MODEL_NAME'].replace('/', '')\n",
    "        save_path = f\"results/retrieval/{now.strftime('%Y-%m-%d_%H-%M-%S')}/{embedding_model_name}/{splitting_strategy}/topk_{top_k}.pkl\"\n",
    "        os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "        df.to_pickle(save_path)\n",
    "\n",
    "for_each_pickle_file(\"data/qa_with_docs_flat\", run_retrieval_eval)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
